# AWE - Agentic Web Explorer Configuration
# ==========================================
# Copy this file to .env and update values

# LLM Settings
# ------------
# Model provider: ollama, groq, openai, anthropic
MODEL_PROVIDER=groq

# ========== MODEL PRESETS ==========
# 
# LARGE MODELS (Fast, no ToT needed):
#   MODEL_NAME=llama-3.3-70b-versatile    # Best quality, 70B params
#   MODEL_NAME=mixtral-8x7b-32768         # Good balance, 46B params
#
# SMALL MODELS (SLMs - ToT recommended):
#   MODEL_NAME=llama-3.1-8b-instant       # Fast SLM, 8B params âš¡
#   MODEL_NAME=gemma2-9b-it               # Good SLM, 9B params
#
# Current model (change based on your needs):
MODEL_NAME=llama-3.1-8b-instant

# Vision model (for screenshot analysis)
VISION_MODEL=llama-3.3-70b-versatile
VISION_ENABLED=true

# Groq API Configuration
GROQ_API_KEY=gsk_your_key_here
GROQ_BASE_URL=https://api.groq.com/openai/v1

# Optional: Ollama API endpoint (if using local models)
# OLLAMA_BASE_URL=http://localhost:11434

# Optional: OpenAI API key (if using OpenAI)
# OPENAI_API_KEY=sk-your-key-here

# Optional: Anthropic API key (if using Claude)
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# ========== TREE OF THOUGHT (ToT) ==========
# ToT enables SLMs to perform like larger models by:
# - Generating multiple extraction strategies
# - Evaluating and scoring each approach
# - Trying strategies in order of confidence
#
TOT_ENABLED=true
TOT_MAX_THOUGHTS=3
TOT_SEARCH_STRATEGY=beam

# Browser Settings
# ----------------
HEADLESS=true
VIEWPORT_WIDTH=1280
VIEWPORT_HEIGHT=720
TIMEOUT=30000

# Server Settings
# ---------------
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# Learning & Knowledge
# --------------------
LEARNING_ENABLED=true
KNOWLEDGE_GRAPH_PATH=./knowledge
SAVE_TEMPLATES=true
TEMPLATES_DIR=./templates

# Reliability
# -----------
MAX_RETRIES=3
REQUEST_DELAY=0.5
MAX_CONCURRENT_PAGES=5
MAX_PAGES=1000

# Logging
# -------
LOG_LEVEL=INFO
VERBOSE=true
SAVE_HTML=true
SCREENSHOTS_ENABLED=true

